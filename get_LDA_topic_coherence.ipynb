{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67519e9",
   "metadata": {},
   "source": [
    "# get_LDA_topic_coherence.ipynb\n",
    "\n",
    "This notebook:\n",
    "* Calculates coherence for the LDA topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9dc51-7454-4087-ac0d-7e8a4c6b928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "df = pd.read_csv(\"./csv/df-all-features.csv\")\n",
    "df = df.drop_duplicates(subset=\"show_uri\", keep=\"first\")\n",
    "df = df.drop(columns=df.columns[df.columns.str.contains(\"Topic\")])  # drop old topic distributions, redoing in this file\n",
    "df[\"transcript\"] = df[\"transcript\"].fillna(\"\")\n",
    "docs = list(df[\"transcript\"])\n",
    "print(docs[0])\n",
    "\n",
    "# remove nltk stopwords from docs\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_documents = [word_tokenize(doc.lower()) for doc in docs]\n",
    "docs = [\n",
    "    [word for word in doc if word.isalnum() and word not in stop_words]\n",
    "    for doc in tokenized_documents\n",
    "]\n",
    "processed_docs = docs \n",
    "dictionary = Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "vectorizer = CountVectorizer(analyzer='word', stop_words='english', lowercase=True)\n",
    "X = vectorizer.fit_transform([' '.join(doc) for doc in processed_docs])\n",
    "\n",
    "COHERENCE_SCORES = []\n",
    "N_TOPICS = [40, 60, 80, 100, 120, 140, 160]\n",
    "for n_topics in N_TOPICS:\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topic_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]  # Top 10 words per topic\n",
    "        topics.append(topic_words)\n",
    "\n",
    "    print(topics)\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(topics=topics, \n",
    "                                         texts=processed_docs, \n",
    "                                         dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    COHERENCE_SCORES.append(coherence_lda)\n",
    "\n",
    "    print(f'Coherence Score: {coherence_lda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff64d7-9201-4ba0-83e3-2fcfb559c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(N_TOPICS)\n",
    "print(COHERENCE_SCORES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bertopic39]",
   "language": "python",
   "name": "conda-env-bertopic39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
